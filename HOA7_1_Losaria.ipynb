{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module 7: Data Wrangling with Pandas\n",
        "\n",
        "## CPE311 Computational Thinking with Python\n",
        "\n",
        "Submitted by: Losaria, Jose Anton Paolo F.\n",
        "\n",
        "Performed on:\n",
        "\n",
        "Submitted on:\n",
        "\n",
        "Submitted to: Engr. Neal Barton James Matira\n",
        "\n"
      ],
      "metadata": {
        "id": "KdTTN7W25txE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.1 Supplementary Activity\n",
        "Using the datasets provided, perform the following exercises:\n",
        "\n",
        "## Exercise 1\n",
        "We want to look at data for the Facebook, Apple, Amazon, Netflix, and Google (FAANG) stocks, but we were given each as a separate CSV file. Combine them into a single file and store the dataframe of the FAANG data as faang for the rest of the exercises:\n",
        "1. Reach each file in.\n",
        "2. Add a column to each dataframe, called ticker, indicating the ticker symbol it is for (Apple's is AAPL, for example). This is how you look up a stock. Each file's name is also the ticker symbol, so be sure to capitalize it.\n",
        "3. Append them together into a single dataframe.\n",
        "4. Save the result in a CSV file called faang.csv\n",
        "\n"
      ],
      "metadata": {
        "id": "Y0uHpbHU6Nbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMy4dlBS7fta",
        "outputId": "155eddfc-f834-4f79-9b40-31a75a4921a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "fb = pd.read_csv('/content/drive/MyDrive/CPE311/HOA7.1/fb.csv')\n",
        "aapl = pd.read_csv('/content/drive/MyDrive/CPE311/HOA7.1/aapl.csv')\n",
        "amzn = pd.read_csv('/content/drive/MyDrive/CPE311/HOA7.1/amzn.csv')\n",
        "nflx = pd.read_csv('/content/drive/MyDrive/CPE311/HOA7.1/nflx.csv')\n",
        "goog = pd.read_csv('/content/drive/MyDrive/CPE311/HOA7.1/goog.csv')\n",
        "\n",
        "fb['ticker'] = 'FB'\n",
        "aapl['ticker'] = 'AAPL'\n",
        "amzn['ticker'] = 'AMZN'\n",
        "nflx['ticker'] = 'NFLX'\n",
        "goog['ticker'] = 'GOOG'\n",
        "\n",
        "faang = pd.concat([fb, aapl, amzn, nflx, goog], ignore_index=True)\n",
        "faang.to_csv('faang.csv', index=False)\n",
        "print(faang.head())"
      ],
      "metadata": {
        "id": "MZF5m2aE6MA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab4af27-6139-4e2c-c67a-4d72f4054055"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         date    open    high       low   close    volume ticker\n",
            "0  2018-01-02  177.68  181.58  177.5500  181.42  18151903     FB\n",
            "1  2018-01-03  181.88  184.78  181.3300  184.67  16886563     FB\n",
            "2  2018-01-04  184.90  186.21  184.0996  184.33  13880896     FB\n",
            "3  2018-01-05  185.59  186.90  184.9300  186.85  13574535     FB\n",
            "4  2018-01-08  187.20  188.90  186.3300  188.28  17994726     FB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "With faang, use type conversion to change the date column into a datetime and the volume column into integers. Then, sort by date and ticker.\n",
        "Find the seven rows with the highest value for volume.\n",
        "Right now, the data is somewhere between long and wide format. Use melt() to make it completely long format. Hint: date and ticker are our ID variables (they uniquely identify each row). We need to melt the rest so that we don't have separate columns for open, high, low, close, and volume."
      ],
      "metadata": {
        "id": "qQj0e2Rm7HQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faang['date'] = pd.to_datetime(faang['date'])\n",
        "faang['volume'] = pd.to_numeric(faang['volume'])\n",
        "faang = faang.sort_values(['date', 'ticker'])\n",
        "seven_highest = faang.nlargest(7, 'volume')\n",
        "faang_melt = (\n",
        "    faang\n",
        "    .melt(\n",
        "        id_vars=['date', 'ticker'],\n",
        "        value_vars=['open', 'high', 'low', 'close', 'volume'],\n",
        "    )\n",
        "    .sort_values(['date', 'ticker'])\n",
        ")\n",
        "print(faang.head())\n",
        "print(seven_highest)\n",
        "print(faang_melt)"
      ],
      "metadata": {
        "id": "Bk0vvVyZ7JEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2216bf96-dc2e-4373-b8f3-d9bad18fab03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           date       open       high        low      close    volume ticker\n",
            "251  2018-01-02   166.9271   169.0264   166.0442   168.9872  25555934   AAPL\n",
            "502  2018-01-02  1172.0000  1190.0000  1170.5100  1189.0100   2694494   AMZN\n",
            "0    2018-01-02   177.6800   181.5800   177.5500   181.4200  18151903     FB\n",
            "1004 2018-01-02  1048.3400  1066.9400  1045.2300  1065.0000   1237564   GOOG\n",
            "753  2018-01-02   196.1000   201.6500   195.4200   201.0700  10966889   NFLX\n",
            "          date      open      high       low     close     volume ticker\n",
            "142 2018-07-26  174.8900  180.1300  173.7500  176.2600  169803668     FB\n",
            "53  2018-03-20  167.4700  170.2000  161.9500  168.1500  129851768     FB\n",
            "57  2018-03-26  160.8200  161.1000  149.0200  160.0600  126116634     FB\n",
            "54  2018-03-21  164.8000  173.4000  163.3000  169.3900  106598834     FB\n",
            "433 2018-09-21  219.0727  219.6482  215.6097  215.9768   96246748   AAPL\n",
            "496 2018-12-21  156.1901  157.4845  148.9909  150.0862   95744384   AAPL\n",
            "463 2018-11-02  207.9295  211.9978  203.8414  205.8755   91328654   AAPL\n",
            "           date ticker variable         value\n",
            "0    2018-01-02   AAPL     open  1.669271e+02\n",
            "1255 2018-01-02   AAPL     high  1.690264e+02\n",
            "2510 2018-01-02   AAPL      low  1.660442e+02\n",
            "3765 2018-01-02   AAPL    close  1.689872e+02\n",
            "5020 2018-01-02   AAPL   volume  2.555593e+07\n",
            "...         ...    ...      ...           ...\n",
            "1254 2018-12-31   NFLX     open  2.601600e+02\n",
            "2509 2018-12-31   NFLX     high  2.701001e+02\n",
            "3764 2018-12-31   NFLX      low  2.600000e+02\n",
            "5019 2018-12-31   NFLX    close  2.676600e+02\n",
            "6274 2018-12-31   NFLX   volume  1.350892e+07\n",
            "\n",
            "[6275 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3\n",
        "Using web scraping, search for the list` of the hospitals, their address and contact information. Save the list in a new csv file, hospitals.csv.\n",
        "Using the generated hospitals.csv, convert the csv file into pandas dataframe. Prepare the data using the necessary preprocessing techniques.\n"
      ],
      "metadata": {
        "id": "6CvS7nuF7JZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "url = \"https://icare.com.ph/icare-accredited-health-partners/\"\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "}\n",
        "response = requests.get(url, headers=headers)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "table = soup.find(\"div\",class_=\"col table-responsive\").find(\"table\")\n",
        "datas = soup.find(\"tbody\").find_all(\"tr\")\n",
        "hospitals = []\n",
        "\n",
        "for data in datas:\n",
        "    columns = data.find_all(\"td\")\n",
        "    if len(columns) >= 8:\n",
        "        name = columns[0].get_text(strip=True)\n",
        "        address = columns[5].get_text(strip=True)\n",
        "        contact = columns[6].get_text(strip=True)\n",
        "        hospitals.append([name, address, contact])\n",
        "\n",
        "with open(\"hospitals.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    write = csv.writer(f)\n",
        "    write.writerow([\"Hospital Name\", \"Address\", \"Contact Information\"])\n",
        "    write.writerows(hospitals)\n",
        "\n",
        "Hospital_PD = pd.DataFrame(hospitals)\n",
        "Hospital_PD = Hospital_PD.fillna(\"Not Available\")\n",
        "Hospital_PD = Hospital_PD.drop_duplicates()"
      ],
      "metadata": {
        "id": "m7p5ScZj7Nu_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "7db69f71-dabb-4d85-ff68-aabf540eb86d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'find'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2236634652.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"col table-responsive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"table\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tbody\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mhospitals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.2 Conclusion:\n",
        "Write your conclusion here.\n"
      ],
      "metadata": {
        "id": "3hhUUPL17Pwj"
      }
    }
  ]
}